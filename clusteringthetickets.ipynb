{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eb9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer,f1_score, accuracy_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Text pre-processing\n",
    "\"\"\"removes punctuation, stopwords, and returns a list of the remaining words, or tokens\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd85a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_tickets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfc91d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ticket_type', 'category', 'sub_category1', 'sub_category2',\n",
       "       'business_service', 'urgency', 'impact'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.select_dtypes([np.number])\n",
    "newdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a20442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined function\n",
    "\n",
    "def content_extractor(content, start=None, end=None):\n",
    "\n",
    "  try:\n",
    "\n",
    "    if start and content and end:\n",
    "\n",
    "      builder=\"{}(.*)(?={})\".format(start,end)\n",
    "\n",
    "      pattern=re.compile(builder)\n",
    "\n",
    "      return pattern.search(content).group(0)\n",
    "\n",
    "    else:\n",
    "\n",
    "      return content\n",
    "\n",
    "  except Exception as e:\n",
    "\n",
    "    return content\n",
    "\n",
    "    \n",
    "\n",
    "ser1=df.apply(lambda x: content_extractor(x,\"start_text\",\"end_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8595f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= pd.DataFrame(df['ticket_type'])\n",
    "X= df.drop(columns=[\"title\",\"ticket_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d9a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27120    tuesday november pm maternity leaver form hell...\n",
       "38261    error hello please be aware had recovery error...\n",
       "37829    error when attempting access hi please escalat...\n",
       "31054    tuesday needed today hi today presentation aft...\n",
       "15808    pm addresses hi please addresses corresponding...\n",
       "                               ...                        \n",
       "21243    hi created order add user accounts created use...\n",
       "45891    sent thursday client portal environment open p...\n",
       "42613    sent tuesday update possible dear colleagues s...\n",
       "43567    sent friday issue hello have problem with bein...\n",
       "2732     hands event hi help setting technical details ...\n",
       "Name: body, Length: 29129, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X['body'], y, test_size=0.4, random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727c64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'there', 'this', 'sample', 'review', 'happens', 'blah', 'contain', 'happened', 'punctuation', 'universal', 'right', 'right', 'contained']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:498: UserWarning: The parameter 'ngram_range' will not be used since 'analyzer' is callable'\n",
      "  warnings.warn(\"The parameter 'ngram_range' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidfconvert---------------- TfidfVectorizer(analyzer=<function text_process at 0x00000233D5C361F0>,\n",
      "                ngram_range=(1, 3), norm=None, smooth_idf=False)\n",
      "X_transformed----------------   (0, 8507)\t2.783724276491034\n",
      "  (0, 8235)\t1.9106934628254955\n",
      "  (0, 7785)\t4.575075165529544\n",
      "  (0, 5910)\t1.9502112244459544\n",
      "  (0, 5900)\t1.2957592802813203\n",
      "  (0, 5348)\t3.607197064864896\n",
      "  (0, 4896)\t11.124923638174858\n",
      "  (0, 4551)\t4.926860124174084\n",
      "  (0, 3742)\t2.655057578407815\n",
      "  (0, 3741)\t2.001116738125641\n",
      "  (0, 3331)\t7.217189378263067\n",
      "  (0, 3187)\t6.114703546570137\n",
      "  (0, 3036)\t3.182672049921332\n",
      "  (1, 8235)\t1.9106934628254955\n",
      "  (1, 6531)\t6.2755432145481915\n",
      "  (1, 5900)\t1.2957592802813203\n",
      "  (1, 5717)\t6.45117578319135\n",
      "  (1, 3741)\t2.001116738125641\n",
      "  (1, 3591)\t5.2018472771446165\n",
      "  (1, 3235)\t5.55917774388624\n",
      "  (1, 2837)\t6.729743622905944\n",
      "  (1, 655)\t6.8850403658212125\n",
      "  (1, 59)\t4.355860892355224\n",
      "  (2, 8224)\t4.378758856448478\n",
      "  (2, 7818)\t4.93560908636732\n",
      "  :\t:\n",
      "  (29127, 412)\t4.0745970102889775\n",
      "  (29127, 38)\t3.9563188025501814\n",
      "  (29127, 8)\t4.592380912627137\n",
      "  (29128, 8962)\t17.280864381756786\n",
      "  (29128, 8877)\t9.598889917133995\n",
      "  (29128, 8530)\t6.560990649198557\n",
      "  (29128, 8169)\t5.197270610117205\n",
      "  (29128, 7863)\t9.333579371438338\n",
      "  (29128, 7632)\t7.18514495827155\n",
      "  (29128, 7627)\t5.141762466407417\n",
      "  (29128, 7428)\t5.546148243595906\n",
      "  (29128, 7364)\t5.232117341447373\n",
      "  (29128, 6631)\t1.898225636675883\n",
      "  (29128, 6170)\t8.389117762597486\n",
      "  (29128, 5670)\t8.234967082770229\n",
      "  (29128, 5334)\t5.435945103462291\n",
      "  (29128, 4935)\t4.718458854597078\n",
      "  (29128, 4442)\t2.5513873154315463\n",
      "  (29128, 3762)\t1.806400342610899\n",
      "  (29128, 3742)\t2.655057578407815\n",
      "  (29128, 3662)\t12.09676180727813\n",
      "  (29128, 2885)\t11.422290033465108\n",
      "  (29128, 2237)\t3.513496441085976\n",
      "  (29128, 1531)\t4.708606558154067\n",
      "  (29128, 635)\t4.344119074478541\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the text\n",
    "\n",
    "import string\n",
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    4. Remove words\n",
    "    '''\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    return [stemmer.lemmatize(word) for word in nopunc]\n",
    "\n",
    "sample_text = \"Hey There! This is a Sample review, which 123happens {blah}%456 to contain happened punctuations universal rights of right contained.\"\n",
    "print(text_process(sample_text))\n",
    "\n",
    "#Vectorisation : -\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfconvert = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer=text_process,ngram_range=(1, 3)).fit(X_train)\n",
    "print(\"tfidfconvert----------------\",tfidfconvert)\n",
    "X_transformed=tfidfconvert.transform(X_train)\n",
    "print(\"X_transformed----------------\",X_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de8ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the test features to sparse matrix\n",
    "test_features = tfidfconvert.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fc17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19420x9189 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 467564 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b018b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=12, n_init=100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering the training sentences with K-means technique\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "modelkmeans = KMeans(n_clusters=12, init='k-means++', n_init=100)\n",
    "modelkmeans.fit(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba28373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = modelkmeans.predict(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f5b710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
